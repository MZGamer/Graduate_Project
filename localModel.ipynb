{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "dic = {}\n",
    "with open('D:/Work/Project/School_Homework/Graduate_Project/predict_Model/reviewLabel/dict.json') as json_file:\n",
    "    dic = json.load(json_file)\n",
    "\n",
    "reverseDic=dict([(value,key) for (key,value) in dic.items()])\n",
    "\n",
    "def decode(encText):\n",
    "  dectext = \"\"\n",
    "  for id in encText:\n",
    "    if id in reverseDic:\n",
    "      dectext += reverseDic[id]\n",
    "    else:\n",
    "      dectext += \"#\"\n",
    "  return dectext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),layers.Dense(embed_dim),] )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim, })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Input, Dropout, Embedding, Flatten,MaxPooling1D,Conv1D,SimpleRNN,LSTM,GRU,Multiply,GlobalMaxPooling1D\n",
    "from keras.layers import Bidirectional,Activation,BatchNormalization,GlobalAveragePooling1D,MultiHeadAttention\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "np.random.seed(0)  # 指定随机数种子\n",
    "#单词索引的最大个数6000，单句话最大长度60\n",
    "top_words=len(dic)\n",
    "max_words=1000    #序列长度\n",
    "embed_dim=32    #嵌入维度\n",
    "num_labels=4   #10分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(top_words=top_words,max_words=max_words,num_labels=num_labels,mode='LSTM',hidden_dim=[64]):\n",
    "    if mode=='RNN':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, input_length=max_words, output_dim=embed_dim, mask_zero=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(SimpleRNN(hidden_dim[0]))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    elif mode=='MLP':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, input_length=max_words, output_dim=embed_dim, mask_zero=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(hidden_dim[0], activation=\"relu\"))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    elif mode=='GRU':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, input_length=max_words, output_dim=embed_dim))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(GRU(hidden_dim[0]))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    elif mode=='CNN':        #一维卷积\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, input_length=max_words, output_dim=embed_dim, mask_zero=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, padding=\"same\",activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(hidden_dim[0], activation=\"relu\"))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    elif mode=='CNN+LSTM':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, input_length=max_words, output_dim=embed_dim))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, padding=\"same\",activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(LSTM(hidden_dim[0]))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(num_labels, activation=\"softmax\"))\n",
    "    elif mode=='BiLSTM':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, input_length=max_words, output_dim=embed_dim))\n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "        model.add(Dense(hidden_dim[0], activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(num_labels, activation='softmax'))\n",
    "    #下面的网络采用Funcional API实现\n",
    "    elif mode=='TextCNN':\n",
    "        inputs = Input(name='inputs',shape=[max_words,], dtype='float64')\n",
    "        ## 词嵌入使用预训练的词向量\n",
    "        layer = Embedding(top_words, input_length=max_words, output_dim=embed_dim)(inputs)\n",
    "        ## 词窗大小分别为3,4,5\n",
    "        cnn1 = Conv1D(32, 3, padding='same', strides = 1, activation='relu')(layer)\n",
    "        cnn1 = MaxPooling1D(pool_size=2)(cnn1)\n",
    "        cnn2 = Conv1D(32, 4, padding='same', strides = 1, activation='relu')(layer)\n",
    "        cnn2 = MaxPooling1D(pool_size=2)(cnn2)\n",
    "        cnn3 = Conv1D(32, 5, padding='same', strides = 1, activation='relu')(layer)\n",
    "        cnn3 = MaxPooling1D(pool_size=2)(cnn3)\n",
    "        # 合并三个模型的输出向量\n",
    "        cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
    "        x = Flatten()(cnn)\n",
    "        x = Dense(hidden_dim[0], activation='relu')(x)\n",
    "        output = Dense(num_labels, activation='softmax')(x)\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    elif mode=='Attention':\n",
    "        inputs = Input(name='inputs',shape=[max_words,], dtype='float64')\n",
    "        x = Embedding(top_words, input_length=max_words, output_dim=embed_dim, mask_zero=True)(inputs)\n",
    "        x = MultiHeadAttention(1, key_dim=embed_dim)(x, x,x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        output = Dense(num_labels, activation='softmax')(x)\n",
    "        model = Model(inputs=[inputs], outputs=output)\n",
    "\n",
    "    elif mode=='MultiHeadAttention':\n",
    "        inputs = Input(name='inputs',shape=[max_words,], dtype='float64')\n",
    "        x = Embedding(top_words, input_length=max_words, output_dim=embed_dim, mask_zero=True)(inputs)\n",
    "        x = MultiHeadAttention(8, key_dim=embed_dim)(x, x,x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        output = Dense(num_labels, activation='softmax')(x)\n",
    "        model = Model(inputs=[inputs], outputs=output)\n",
    "\n",
    "    elif mode=='Attention+BiLSTM':\n",
    "        inputs = Input(name='inputs',shape=[max_words,], dtype='float64')\n",
    "        x = Embedding(top_words, input_length=max_words, output_dim=embed_dim)(inputs)\n",
    "        x = MultiHeadAttention(2, key_dim=embed_dim)(x, x,x)\n",
    "        x = Bidirectional(LSTM(hidden_dim[0]))(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        output = Dense(num_labels, activation='softmax')(x)\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    elif mode=='BiGRU+Attention':\n",
    "        inputs = Input(name='inputs',shape=[max_words,], dtype='float64')\n",
    "        x = Embedding(top_words, input_length=max_words, output_dim=embed_dim)(inputs)\n",
    "        x = Bidirectional(GRU(32,return_sequences=True))(x)\n",
    "        x = MultiHeadAttention(2, key_dim=embed_dim)(x,x,x)\n",
    "        x = Bidirectional(GRU(32))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        output = Dense(num_labels, activation='softmax')(x)\n",
    "        model = Model(inputs=[inputs], outputs=output)\n",
    "\n",
    "    elif mode=='Transformer':\n",
    "        inputs = Input(name='inputs',shape=[max_words,], dtype='float64')\n",
    "        x = Embedding(top_words, input_length=max_words, output_dim=embed_dim, mask_zero=True)(inputs)\n",
    "        x = TransformerEncoder(embed_dim, 32, 4)(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outputs = Dense(num_labels, activation='softmax')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "    elif mode=='PositionalEmbedding+Transformer':\n",
    "        inputs = Input(name='inputs',shape=[max_words,], dtype='float64')\n",
    "        x= PositionalEmbedding(sequence_length=max_words, input_dim=top_words, output_dim=embed_dim)(inputs)\n",
    "        x = TransformerEncoder(embed_dim, 32, 4)(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outputs = Dense(num_labels, activation='softmax')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失和精度的图,和混淆矩阵指标等等\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_loss(history):\n",
    "    # 显示训练和验证损失图表\n",
    "    plt.subplots(1,2,figsize=(10,3))\n",
    "    plt.subplot(121)\n",
    "    loss = history.history[\"loss\"]\n",
    "    epochs = range(1, len(loss)+1)\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    plt.plot(epochs, loss, \"bo\", label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    plt.plot(epochs, acc, \"b-\", label=\"Training Acc\")\n",
    "    plt.plot(epochs, val_acc, \"r--\", label=\"Validation Acc\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_confusion_matrix(model,X_test,Y_test_original):\n",
    "    #dic2 = {0:\"Not_Relative\", 1:\"Very_Negative\", 2:\"Negative\", 3:\"Nature\", 4:\"Positive\", 5:\"Very_Positive\"}\n",
    "    dic2 = {0:\"Not_Relative\", 1:\"Negative\", 2:\"Nature\", 3:\"Positive\"}\n",
    "    #预测概率\n",
    "    prob=model.predict(X_test)\n",
    "    #预测类别\n",
    "    pred=np.argmax(prob,axis=1)\n",
    "    #数据透视表，混淆矩阵\n",
    "    pred=pd.Series(pred).map(dic2)\n",
    "    Y_test_original=pd.Series(Y_test_original).map(dic2)\n",
    "    table = pd.crosstab(Y_test_original, pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "    #print(table)\n",
    "    sns.heatmap(table,cmap='Blues',fmt='.20g', annot=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #计算混淆矩阵的各项指标\n",
    "    print(classification_report(Y_test_original, pred))\n",
    "    #科恩Kappa指标\n",
    "    print('科恩Kappa'+str(cohen_kappa_score(Y_test_original, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义训练函数\n",
    "def train_fuc(max_words=max_words,mode='BiLSTM+Attention',batch_size=32,epochs=10,hidden_dim=[32],show_loss=True,show_confusion_matrix=True):\n",
    "    #构建模型\n",
    "    model=build_model(max_words=max_words,mode=mode)\n",
    "    print(model.summary())\n",
    "    es = EarlyStopping(patience=5)\n",
    "    with tf.device('/GPU:0'):\n",
    "      history=model.fit(X_train, Y_train,batch_size=batch_size,epochs=epochs,validation_split=0.2, verbose=1,callbacks=[es])\n",
    "    print('——————————-----------------——訓練完成—————-----------------------------———————')\n",
    "    # 评估模型\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "    print(\"val DATA ACC: = {:.4f}\".format(accuracy))\n",
    "\n",
    "    if show_loss:\n",
    "        plot_loss(history)\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        plot_confusion_matrix(model=model,X_test=X_test,Y_test_original=Y_test_original)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def shuffle(Question, Answer, split_size):\n",
    "  x_train = []\n",
    "  x_val = []\n",
    "  y_train = []\n",
    "  y_val = []\n",
    "\n",
    "  trainSize = []\n",
    "  valSize = []\n",
    "\n",
    "  trainCount = []\n",
    "  valCount = []\n",
    "  dataSize = pd.Series(Answer).value_counts()\n",
    "  print(dataSize)\n",
    "  for i in range(len(dataSize)):\n",
    "    trainSize.append(dataSize[i] * (1 - split_size))\n",
    "    valSize.append(dataSize[i] - trainSize[i])\n",
    "    trainCount.append(0)\n",
    "    valCount.append(0)\n",
    "\n",
    "  for i in range(len(Question)):\n",
    "    dice = random.random()\n",
    "    choose = 0\n",
    "    if(dice <= split_size):\n",
    "      choose = 1\n",
    "\n",
    "    if(choose == 0):\n",
    "      if(trainCount[Answer[i]] < trainSize[Answer[i]]):\n",
    "        trainCount[Answer[i]] += 1\n",
    "        x_train.append(Question[i])\n",
    "        y_train.append(Answer[i])\n",
    "      else:\n",
    "        valCount[Answer[i]] += 1\n",
    "        x_val.append(Question[i])\n",
    "        y_val.append(Answer[i])\n",
    "    elif(choose == 1):\n",
    "      if(valCount[Answer[i]] < valCount[Answer[i]]):\n",
    "        valCount[Answer[i]] += 1\n",
    "        x_val.append(Question[i])\n",
    "        y_val.append(Answer[i])\n",
    "      else:\n",
    "        trainCount[Answer[i]] += 1\n",
    "        x_train.append(Question[i])\n",
    "        y_train.append(Answer[i])\n",
    "  print(pd.Series(y_train).value_counts())\n",
    "  print(pd.Series(y_val).value_counts())\n",
    "  return x_train, x_val, y_train, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model Create函数\n",
    "def create_model(max_words=max_words,mode='BiLSTM+Attention',batch_size=32,epochs=10,hidden_dim=[32],show_loss=True,show_confusion_matrix=True):\n",
    "    #构建模型\n",
    "    model=build_model(max_words=max_words,mode=mode)\n",
    "    print(model.summary())\n",
    "    es = EarlyStopping(patience=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 1000)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posi  (None, 1000, 32)          132608    \n",
      " tionalEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, 1000, 32)          19040     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 32)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151780 (592.89 KB)\n",
      "Trainable params: 151780 (592.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_words=len(dic)\n",
    "max_words=1000\n",
    "batch_size=16\n",
    "epochs=20\n",
    "show_confusion_matrix=True\n",
    "show_loss=True\n",
    "mode='PositionalEmbedding+Transformer'\n",
    "model = create_model(mode=mode,batch_size=batch_size,epochs=epochs,show_confusion_matrix=show_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"D:/Work/Project/School_Homework/Graduate_Project/predict_Model/reviewLabel/binFoodQualityV1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 1000)]            0         \n",
      "                                                                 \n",
      " positional_embedding_6 (Po  (None, 1000, 32)          132608    \n",
      " sitionalEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_encoder_6 (Tra  (None, 1000, 32)          19040     \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_6 (Gl  (None, 32)                0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151780 (592.89 KB)\n",
      "Trainable params: 151780 (592.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2547\n",
      "7451\n",
      "701\n",
      "0    701\n",
      "3    701\n",
      "1    597\n",
      "2    548\n",
      "Name: count, dtype: int64\n",
      "0    593\n",
      "3    590\n",
      "1    503\n",
      "2    467\n",
      "Name: count, dtype: int64\n",
      "3    111\n",
      "0    108\n",
      "1     94\n",
      "2     81\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import re\n",
    "import math\n",
    "from pylab import mpl\n",
    "\n",
    "TARGET = 'food_quality'\n",
    "\n",
    "maxlen = 1000\n",
    "vocab_size = len(dic)\n",
    "# 讀取CSV文件並轉換為DataFrame\n",
    "df = pd.read_csv('reviewTypeClean.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "texts = df['Comment'].tolist()\n",
    "\n",
    "full_size_labels = []\n",
    "full_size_encText = []\n",
    "\n",
    "labels = []\n",
    "encText = []\n",
    "\n",
    "Label0Max = 700\n",
    "Label0Count = 0\n",
    "\n",
    "\n",
    "PositiveMax = 700\n",
    "PositiveCount = 0\n",
    "for t in range(len(texts)):\n",
    "  texts[t] = emoji.demojize(texts[t])\n",
    "  texts[t] = re.sub(':\\S+?:', ' ', texts[t])\n",
    "  encs = []\n",
    "\n",
    "\n",
    "  if len(texts[t]) > maxlen:\n",
    "    texts[t] = texts[:maxlen]\n",
    "  for i in range(maxlen):\n",
    "    if(i<len(texts[t]) and texts[t][i] in dic):\n",
    "      encs.append(dic[texts[t][i]])\n",
    "    else:\n",
    "      encs.append(0)\n",
    "\n",
    "  if (encs != None) :\n",
    "    full_size_encText.append(encs)\n",
    "    if(df.iloc[t][TARGET] == 1 or df.iloc[t][TARGET] == 2):\n",
    "      full_size_labels.append(1)\n",
    "    elif(df.iloc[t][TARGET] == 4 or df.iloc[t][TARGET] == 5):\n",
    "      full_size_labels.append(3)\n",
    "    elif(df.iloc[t][TARGET] == 3):\n",
    "      full_size_labels.append(2)\n",
    "    else:\n",
    "      full_size_labels.append(df.iloc[t][TARGET]) \n",
    "\n",
    "\n",
    "  if (df.iloc[t][TARGET] == 0):\n",
    "    if(Label0Count > Label0Max):\n",
    "      continue\n",
    "    Label0Count += 1\n",
    "\n",
    "  if(df.iloc[t][TARGET] == 1 or df.iloc[t][TARGET] == 2):\n",
    "    labels.append(1)\n",
    "  elif(df.iloc[t][TARGET] == 4 or df.iloc[t][TARGET] == 5):\n",
    "    if(PositiveCount > PositiveMax):\n",
    "      continue\n",
    "\n",
    "    PositiveCount += 1\n",
    "    labels.append(3)\n",
    "  elif(df.iloc[t][TARGET] == 3):\n",
    "    labels.append(2)\n",
    "  else:\n",
    "    labels.append(df.iloc[t][TARGET]) \n",
    "  #labels.append(df.iloc[t][TARGET])\n",
    "\n",
    "  '''if (df.iloc[t][TARGET] == 1):\n",
    "    labels.append(0)\n",
    "  elif (df.iloc[t][TARGET] == -1):\n",
    "    labels.append(1)\n",
    "  else:\n",
    "    labels.append(2)'''\n",
    "\n",
    "  encText.append(encs)\n",
    "\n",
    "\n",
    "\n",
    "print(len(labels))\n",
    "print(len(texts))\n",
    "print(pd.Series(labels).value_counts()[0])\n",
    "pd.Series(labels).value_counts().plot(kind='bar')\n",
    "# 將資料集分割成訓練集和測試集\n",
    "x_train, x_val, y_train, y_val = shuffle(encText, labels, 0.2)\n",
    "# 將標籤轉換為模型所需的格式\n",
    "label_dict = {label: i for i, label in enumerate(set(labels))}\n",
    "\n",
    "\n",
    "y_train = tf.constant([label_dict[label] for label in y_train], dtype=tf.int32)  # 將資料型別轉換為 tf.int32\n",
    "y_val = tf.constant([label_dict[label] for label in y_val], dtype=tf.int32)  # 將資料型別轉換為 tf.int32\n",
    "num_classes = len(label_dict)\n",
    "\n",
    "Y_train = tf.one_hot(y_train, num_classes)\n",
    "\n",
    "Y_test_original=y_val\n",
    "Y_test = tf.one_hot(y_val, num_classes)\n",
    "\n",
    "X_train = tf.constant(x_train)\n",
    "X_test = tf.constant(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 指定使用的GPU索引\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 208s 892ms/step\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    ans = model.predict(full_size_encText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lebels 0's acc analyze:\n",
      "predict:[1141, 61, 52, 189, 0, 0]\n",
      "Total: 1443\n",
      "Acc: 0.7907137907137907 \n",
      "\n",
      "lebels 1's acc analyze:\n",
      "predict:[63, 394, 66, 74, 0, 0]\n",
      "Total: 597\n",
      "Acc: 0.6599664991624791 \n",
      "\n",
      "lebels 2's acc analyze:\n",
      "predict:[59, 30, 352, 107, 0, 0]\n",
      "Total: 548\n",
      "Acc: 0.6423357664233577 \n",
      "\n",
      "lebels 3's acc analyze:\n",
      "predict:[670, 210, 468, 3515, 0, 0]\n",
      "Total: 4863\n",
      "Acc: 0.7228048529714168 \n",
      "\n",
      "Total: \n",
      "[[1141, 61, 52, 189, 0, 0], [63, 394, 66, 74, 0, 0], [59, 30, 352, 107, 0, 0], [670, 210, 468, 3515, 0, 0]]\n",
      "0.7250033552543282\n",
      "Alert\n"
     ]
    }
   ],
   "source": [
    "chk = [[0,0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0]]\n",
    "''',[0,0,0,0,0,0],[0,0,0,0,0,0]]'''\n",
    "correct = 0\n",
    "positive = [4,5]\n",
    "negative = [1,2]\n",
    "nature = [3]\n",
    "\n",
    "checkLabel = full_size_labels\n",
    "alert = []\n",
    "for i in range(len(checkLabel)):\n",
    "  chk[checkLabel[i]][np.argmax(ans[i])] += 1\n",
    "  if checkLabel[i] == np.argmax(ans[i]):\n",
    "    correct += 1\n",
    "  if(checkLabel[i] in negative and np.argmax(ans[i]) in positive):\n",
    "    alert.append(i)\n",
    "\n",
    "for label in range(len(chk)):\n",
    "  print(f\"lebels {label}'s acc analyze:\")\n",
    "  print(f\"predict:{chk[label]}\")\n",
    "  print(f\"Total: {sum(chk[label])}\")\n",
    "  print(f\"Acc: {chk[label][label] / sum(chk[label])} \\n\")\n",
    "\n",
    "print(\"Total: \")\n",
    "print(chk)\n",
    "print(correct / len(checkLabel))\n",
    "\n",
    "print(\"Alert\")\n",
    "for i in alert:\n",
    "  print(decode(encText[i]))\n",
    "  print(f\"Ans:{checkLabel[i]}, Pred:{np.argmax(ans[i])}\")\n",
    "  print(ans[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(string):\n",
    "    string = emoji.demojize(string)\n",
    "    string = re.sub(':\\S+?:', ' ', string)\n",
    "    encs = []\n",
    "    if len(string) > maxlen:\n",
    "        string = string[:maxlen]\n",
    "\n",
    "    for i in range(maxlen):\n",
    "        if(i<len(string) and string[i] in dic):\n",
    "            encs.append(dic[string[i]])\n",
    "        else:\n",
    "            encs.append(0)\n",
    "    if (encs != None) :\n",
    "        return encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "[[4.3960146e-05 9.4098705e-01 1.8116778e-02 4.0852193e-02]]\n",
      "Pred:1\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import re\n",
    "maxlen = 1000\n",
    "testCase = \"匈牙利海鮮焗飯還ok算好吃(雖然裡面有薄片杏包菇)，青醬不推薦完全不香又比較稀，濃湯也味道太清淡，有點小失望，飲料還ok紅茶蠻香，胚芽奶也真 的又胚芽真材實料，但這個要喝全糖不能微糖不然天然的胚芽味道有點重唷，整個場所芳香劑味道真的太重鼻子一直過敏\"\n",
    "encText = []\n",
    "encs = []\n",
    "encText.append(encode(testCase))\n",
    "print(len(encText))\n",
    "predictions = model.predict(encText)\n",
    "print(predictions)\n",
    "print(f\"Pred:{np.argmax(predictions[0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
